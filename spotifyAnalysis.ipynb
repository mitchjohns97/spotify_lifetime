{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To do:\n",
    "4. Calls to all info for audio data\n",
    "6. Chart songs, albums, artists over time\n",
    "7. Clean up unique ids for compilations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "\n",
    "!pip install pandas matplotlib seaborn spotipy requests\n",
    "print(\"Installed all necessary packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810b676-cbf6-4472-9b3c-7c0742d8cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import base64\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "\n",
    "print(\"Imported necessary libraries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811686fa-7eb0-4415-aac3-8432475e0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all accounts, combine all json files in the export folder\n",
    "\n",
    "paths = ['mmcmm', 'thing21']\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "    # Path to the folder containing JSON files\n",
    "    folder_path = 'rd/' + path + '/'  \n",
    "    \n",
    "    # List all JSON files in the folder\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Loop through each JSON file and read it into a DataFrame\n",
    "    for file in json_files:\n",
    "        try:\n",
    "            df = pd.read_json(file)  # Read JSON file into a DataFrame\n",
    "            dataframes.append(df)    # Append the DataFrame to the list\n",
    "        except ValueError as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Output the final DataFrame to a CSV file\n",
    "    output_file = 'outputs/' + path + 'streams.csv'  \n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"{path} data saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91671fc2-3e9d-4ee9-a66b-efea23036a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean and combine data for all accounts\n",
    "\n",
    "mmcmmdata = pd.read_csv('outputs/mmcmmstreams.csv', low_memory=False)\n",
    "thing21data = pd.read_csv('outputs/thing21streams.csv', low_memory=False)\n",
    "\n",
    "alldata = [mmcmmdata, thing21data]\n",
    "names = ['mmcmm', 'thing21']\n",
    "\n",
    "for idx, data in enumerate(alldata):\n",
    "    # Perform data cleaning and transformations\n",
    "    del data['user_agent_decrypted']\n",
    "    data['ts'] = pd.to_datetime(data['ts'])\n",
    "    data['skipped'] = data['skipped'].astype(bool)\n",
    "    data['offline'] = data['offline'].astype(bool)\n",
    "    data['incognito_mode'] = data['incognito_mode'].astype(bool)\n",
    "    data['offline_timestamp'] = data['offline_timestamp'].replace([float('inf'), float('-inf')], None)\n",
    "    data['offline_timestamp'] = data['offline_timestamp'].fillna(0)\n",
    "    data['offline_timestamp'] = data['offline_timestamp'].astype(int)\n",
    "    data['offline_timestamp'] = pd.to_datetime(data['offline_timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "streams = pd.concat(alldata, ignore_index=True)\n",
    "\n",
    "# Output the final DataFrame to a CSV file\n",
    "output_name = 'outputs/combined.csv'  \n",
    "streams.to_csv(output_name, index=False)\n",
    "\n",
    "print(f\"Combined CSV of all streams saved as {output_name}. Used in code as streams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96351c17-ebf7-4e82-8ce5-5e1b6c06549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lifetime rankings by streams\n",
    "\n",
    "# Top Artists by Play Count\n",
    "top_artists = streams['master_metadata_album_artist_name'].value_counts().head(20)\n",
    "# print(\"Top 20 Artists by Play Count:\")\n",
    "# print(top_artists)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "top_artists.plot(kind='bar')\n",
    "plt.title(\"Top 20 Artists by Number of Streams\")\n",
    "plt.show()\n",
    "\n",
    "# Top Albums by Play Count\n",
    "top_albums = streams['master_metadata_album_album_name'].value_counts().head(20)\n",
    "# print(\"Top 20 Albums by Play Count:\")\n",
    "# print(top_albums)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "top_albums.plot(kind='bar')\n",
    "plt.title(\"Top 20 Albums by Number of Streams\")\n",
    "plt.show()\n",
    "\n",
    "# Top Songs by Play Count\n",
    "top_songs = streams['master_metadata_track_name'].value_counts().head(20)\n",
    "# print(\"Top 20 Songs by Play Count:\")\n",
    "# print(top_songs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "top_songs.plot(kind='bar')\n",
    "plt.title(\"Top 20 Songs by Number of Streams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b305d-b5bf-47ee-9f97-98ba2fd6f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifetime rankings by duration\n",
    "\n",
    "# Top Artists by Total Play Duration\n",
    "top_artists = streams.groupby('master_metadata_album_artist_name')['ms_played'].sum().div(3600000).sort_values(ascending=False).head(20)\n",
    "# print(\"Top 20 Artists by Total Play Duration (hours):\")\n",
    "# print(top_artists)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_artists.plot(kind='bar')\n",
    "plt.title(\"Top 20 Artists by Total Play Duration\")\n",
    "plt.xlabel(\"Artist\")\n",
    "plt.ylabel(\"Play Duration (hours)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top Albums by Total Play Duration\n",
    "top_albums = streams.groupby('master_metadata_album_album_name')['ms_played'].sum().div(3600000).sort_values(ascending=False).head(20)\n",
    "# print(\"Top 20 Albums by Total Play Duration (hours):\")\n",
    "# print(top_albums)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_albums.plot(kind='bar')\n",
    "plt.title(\"Top 20 Albums by Total Play Duration\")\n",
    "plt.xlabel(\"Album\")\n",
    "plt.ylabel(\"Play Duration (hours)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top Songs by Total Play Duration\n",
    "top_songs = streams.groupby('master_metadata_track_name')['ms_played'].sum().div(3600000).sort_values(ascending=False).head(20)\n",
    "# print(\"Top 20 Songs by Total Play Duration (hours):\")\n",
    "# print(top_songs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_songs.plot(kind='bar')\n",
    "plt.title(\"Top 20 Songs by Total Play Duration\")\n",
    "plt.xlabel(\"Song\")\n",
    "plt.ylabel(\"Play Duration (hours)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed487e-64dc-4876-8b50-6c776d5333d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total play over lifetime\n",
    "\n",
    "# Extracting time components\n",
    "streams['year'] = streams['ts'].dt.year\n",
    "streams['month'] = streams['ts'].dt.month\n",
    "\n",
    "# Calculate total duration in hours per year\n",
    "streams['hours_played'] = streams['ms_played'] / (1000 * 60 * 60)  # Convert ms to hours\n",
    "yearly_duration = streams.groupby('year')['hours_played'].sum()\n",
    "\n",
    "# Plotting the number of streams per year\n",
    "yearly_counts = streams.groupby('year').size()\n",
    "plt.figure(figsize=(10, 6))\n",
    "yearly_counts.plot(marker='o', label='Number of Streams', color='blue')\n",
    "plt.title(\"Number of Streams and Total Play Duration per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count of Streams\")\n",
    "\n",
    "# Plotting total play duration per year on a secondary y-axis\n",
    "ax = plt.gca()\n",
    "ax2 = ax.twinx()\n",
    "yearly_duration.plot(marker='s', linestyle='--', label='Total Play Duration (hours)', color='green', ax=ax2)\n",
    "ax2.set_ylabel(\"Total Duration (hours)\")\n",
    "\n",
    "# Combine legends\n",
    "lines_1, labels_1 = ax.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39d26c-7924-4eb4-a262-528e0b8b0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get unique track ids from streams\n",
    "track_uris = streams['spotify_track_uri'].dropna().astype(str).unique().tolist()\n",
    "track_ids = [uri.split(':')[-1] for uri in track_uris if 'spotify:track:' in uri]\n",
    "\n",
    "if not track_ids:\n",
    "    print(\"No valid track IDs found.\")\n",
    "else:\n",
    "    print(f\"Found {len(track_ids)} unique track IDs.\")\n",
    "\n",
    "# 2. Reference token as my access token\n",
    "token_code = token['access_token']\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token_code}\"\n",
    "}\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "output_file = \"track_data.csv\"\n",
    "all_results = []\n",
    "\n",
    "# 3. Make batch calls to the tracks API\n",
    "# We'll use: GET https://api.spotify.com/v1/tracks?ids=...\n",
    "for i, track_chunk in enumerate(chunks(track_ids, 50)):\n",
    "    # Construct the request\n",
    "    url = \"https://api.spotify.com/v1/tracks\"\n",
    "    params = {\n",
    "        \"ids\": \",\".join(track_chunk)\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Request failed with status {response.status_code}. Retrying in 30s...\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    \n",
    "    data = response.json()\n",
    "    tracks = data.get('tracks', [])\n",
    "    \n",
    "    # Extract desired fields from each track object\n",
    "    for t in tracks:\n",
    "        if t:\n",
    "            track_info = {\n",
    "                \"track_id\": t.get(\"id\"),\n",
    "                \"track_name\": t.get(\"name\"),\n",
    "                \"album_name\": t.get(\"album\", {}).get(\"name\"),\n",
    "                \"album_release_date\": t.get(\"album\", {}).get(\"release_date\"),\n",
    "                \"album_id\": t.get(\"album\", {}).get(\"id\"),\n",
    "                \"album_type\": t.get(\"album\", {}).get(\"album_type\"),\n",
    "                \"artist_names\": [artist[\"name\"] for artist in t.get(\"artists\", [])],\n",
    "                \"artist_ids\": [artist[\"id\"] for artist in t.get(\"artists\", [])],\n",
    "                \"track_popularity\": t.get(\"popularity\"),\n",
    "                \"duration_ms\": t.get(\"duration_ms\"),\n",
    "                \"explicit\": t.get(\"explicit\"),\n",
    "                \"isrc\": t.get(\"external_ids\", {}).get(\"isrc\"),\n",
    "                \"track_uri\": t.get(\"uri\")\n",
    "            }\n",
    "            all_results.append(track_info)\n",
    "    \n",
    "    # small delay to reduce risk of rate limits\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # 4. Save data intermittently (every 10 chunks)\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        temp_df = pd.DataFrame(all_results)\n",
    "        temp_df.to_csv(output_file, mode='a', header=not i, index=False)\n",
    "        all_results = []\n",
    "        print(f\"Processed {i*50} tracks and saved to file.\")\n",
    "\n",
    "# Save any remaining results after the loop completes\n",
    "if all_results:\n",
    "    final_df = pd.DataFrame(all_results)\n",
    "    # If we never wrote before, we need a header; if we did, no header is necessary\n",
    "    # We'll include the header if it's the first time writing\n",
    "    # Check if file exists:\n",
    "    import os\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    final_df.to_csv(output_file, mode='a', header=write_header, index=False)\n",
    "\n",
    "print(\"Data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86e757-c89f-4e84-a953-ab326291f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Enhance data\n",
    "\n",
    "# Establish API connection\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "client_id = \"id\"\n",
    "client_secret = \"secret\"\n",
    "\n",
    "# Set up authorization\n",
    "auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "token = auth_manager.get_access_token()\n",
    "\n",
    "print(f\"Established connection to Spotipy API. \\nToken is {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f724b-c17c-418c-a8d9-ca10b91d4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If library.csv exists, load it; otherwise create an empty DataFrame\n",
    "\n",
    "track_info_file = \"library.csv\"\n",
    "\n",
    "if os.path.exists(track_info_file):\n",
    "    track_info_df = pd.read_csv(track_info_file)\n",
    "else:\n",
    "    track_info_df = pd.DataFrame(columns=[\n",
    "        'track_id', 'track_name', 'album_name', 'album_release_date', 'album_id', 'album_type',\n",
    "        'artist_names', 'artist_ids', 'track_popularity', 'duration_ms', 'explicit', 'isrc',\n",
    "        'ean', 'upc', 'external_urls', 'track_uri'\n",
    "    ])\n",
    "\n",
    "print(\"Current trackInfo DataFrame:\")\n",
    "display(track_info_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17647034-3c0c-4ad5-85f0-3a328c7f8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract track IDs from spotify_track_uri field\n",
    "track_uris = streams['spotify_track_uri'].dropna().astype(str).unique().tolist()\n",
    "track_ids = [uri.split(':')[-1] for uri in track_uris if 'spotify:track:' in uri]\n",
    "\n",
    "print(f\"Found {len(track_ids)} unique track IDs in streams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b530f5-db5a-40a5-bcba-d8988a34263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which track IDs are already known\n",
    "known_ids = set(track_info_df['track_id'].dropna())\n",
    "new_ids = [t for t in track_ids if t not in known_ids]\n",
    "\n",
    "print(f\"{len(new_ids)} track IDs are missing from trackInfo and need enrichment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0be3e-0af2-42b5-8227-dcc06bf49442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Missing Track Info from the Spotify API\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token['access_token']}\"\n",
    "}\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "new_data = []\n",
    "url = \"https://api.spotify.com/v1/tracks\"\n",
    "\n",
    "for i, chunk in enumerate(chunks(new_ids, 50)):\n",
    "    params = {\"ids\": \",\".join(chunk)}\n",
    "    r = requests.get(url, headers=headers, params=params)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Request failed with status {r.status_code}, waiting 30s before retrying...\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    response_data = r.json()\n",
    "    tracks = response_data.get('tracks', [])\n",
    "    for t in tracks:\n",
    "        if t:\n",
    "            new_data.append({\n",
    "                \"track_id\": t.get(\"id\"),\n",
    "                \"track_name\": t.get(\"name\"),\n",
    "                \"album_name\": t.get(\"album\", {}).get(\"name\"),\n",
    "                \"album_release_date\": t.get(\"album\", {}).get(\"release_date\"),\n",
    "                \"album_id\": t.get(\"album\", {}).get(\"id\"),\n",
    "                \"album_type\": t.get(\"album\", {}).get(\"album_type\"),\n",
    "                \"artist_names\": [artist[\"name\"] for artist in t.get(\"artists\", [])],\n",
    "                \"artist_ids\": [artist[\"id\"] for artist in t.get(\"artists\", [])],\n",
    "                \"track_popularity\": t.get(\"popularity\"),\n",
    "                \"duration_ms\": t.get(\"duration_ms\"),\n",
    "                \"explicit\": t.get(\"explicit\"),\n",
    "                \"isrc\": t.get(\"external_ids\", {}).get(\"isrc\"),\n",
    "                \"ean\": t.get(\"external_ids\", {}).get(\"ean\"),\n",
    "                \"upc\": t.get(\"external_ids\", {}).get(\"upc\"),\n",
    "                \"external_urls\": t.get(\"external_urls\", {}),\n",
    "                \"track_uri\": t.get(\"uri\")\n",
    "            })\n",
    "    # small delay to reduce rate limit risks\n",
    "    time.sleep(0.2)\n",
    "\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "print(f\"Fetched metadata for {len(new_data_df)} new tracks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed66f1a-d3bd-4221-b4b5-2c3dab7fe4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update new data to library\n",
    "if not new_data_df.empty:\n",
    "    # Write headers if library.csv was empty before\n",
    "    write_header = (track_info_df.empty and not os.path.exists(track_info_file)) or (not os.path.exists(track_info_file))\n",
    "    new_data_df.to_csv(track_info_file, mode='a', header=write_header, index=False)\n",
    "    print(f\"Appended new track metadata to {track_info_file}\")\n",
    "    \n",
    "    # Reload track_info_df to include newly added data\n",
    "    track_info_df = pd.read_csv(track_info_file)\n",
    "else:\n",
    "    print(\"No new track data to append.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b31de3-52a1-42e6-b1bd-2052c91162ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge streams with library\n",
    "\n",
    "# Ensure streams has a track_id column for merging\n",
    "streams['track_id'] = streams['spotify_track_uri'].astype(str).apply(\n",
    "    lambda x: x.split(':')[-1] if 'spotify:track:' in x else x\n",
    ")\n",
    "\n",
    "enriched_streams = streams.merge(track_info_df, on='track_id', how='left')\n",
    "\n",
    "print(\"Enriched streams DataFrame:\")\n",
    "display(enriched_streams.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
