{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To do:\n",
    "5. merge api data back to streams (columns as lists for artist data)\n",
    "7. reference ids to avoid dupe name issue\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 0: Install and import packages\n",
    "\n",
    "# 0.1 Install necessary packages\n",
    "\n",
    "!pip install pandas matplotlib seaborn spotipy requests\n",
    "print(\"Installed all necessary packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810b676-cbf6-4472-9b3c-7c0742d8cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 Import needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import base64\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "\n",
    "print(\"Imported necessary libraries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811686fa-7eb0-4415-aac3-8432475e0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Load and clean data\n",
    "\n",
    "# 1.1 For all accounts, combine all json files\n",
    "\n",
    "# Move all uncompressed export folders into a folder titled 'rd'. 'rd' should exist beside this .ipynb file. \n",
    "# Keep all .json files inside account folder.\n",
    "\n",
    "accounts = ['mmcmm', 'thing21', 'danDrust', 'michaelBesch', 'walter'] # Include names of all account folders you want to analyze\n",
    "\n",
    "prefix = \"Streaming_History\"\n",
    "\n",
    "alldata = []\n",
    "\n",
    "for path in accounts:\n",
    "\n",
    "    # Path to the folder containing JSON files\n",
    "    folder_path = 'rd/' + path + '/'  \n",
    "    \n",
    "    # List all Streaming History JSON files in the folder\n",
    "    pattern = f\"{prefix}*.json\"\n",
    "    json_files = glob.glob(os.path.join(folder_path, pattern))\n",
    "    \n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Loop through each JSON file and read it into a DataFrame\n",
    "    for file in json_files:\n",
    "        try:\n",
    "            df = pd.read_json(file)  # Read JSON file into a DataFrame\n",
    "            dataframes.append(df)    # Append the DataFrame to the list\n",
    "        except ValueError as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Output the final DataFrame to a CSV file\n",
    "    output_file = 'outputs/' + path + 'Streams.csv'  \n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"{path} data saved as {output_file}\")\n",
    "    \n",
    "    # Clean and transform account data for use\n",
    "    final_df['ts'] = pd.to_datetime(final_df['ts'])\n",
    "    final_df['skipped'] = final_df['skipped'].astype(bool)\n",
    "    final_df['offline'] = final_df['offline'].astype(bool)\n",
    "    final_df['incognito_mode'] = final_df['incognito_mode'].astype(bool)\n",
    "    final_df['offline_timestamp'] = final_df['offline_timestamp'].replace([float('inf'), float('-inf')], None)\n",
    "    final_df['offline_timestamp'] = final_df['offline_timestamp'].fillna(0)\n",
    "    final_df['offline_timestamp'] = final_df['offline_timestamp'].astype(int)\n",
    "    final_df['offline_timestamp'] = pd.to_datetime(final_df['offline_timestamp'], unit='s', errors='coerce')\n",
    "\n",
    "    # Use alldata to collect data from all accounts\n",
    "    alldata.append(final_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "streams = pd.concat(alldata, ignore_index=True)\n",
    "\n",
    "# Output the final DataFrame to a CSV file\n",
    "output_name = 'outputs/combined.csv'  \n",
    "streams.to_csv(output_name, index=False)\n",
    "\n",
    "print(f\"Combined CSV of all streams saved as {output_name}. Used in code as streams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86e757-c89f-4e84-a953-ab326291f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Enhance data\n",
    "\n",
    "# 2.1 Establish API connection\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "client_id = \"\" # message mitch for keys if wanted\n",
    "client_secret = \"\"\n",
    "\n",
    "# Set up authorization\n",
    "auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "token = auth_manager.get_access_token()\n",
    "\n",
    "print(f\"Established connection to Spotipy API. \\nToken is {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9329e-766e-41c0-95cc-c7d8b384a99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To do: Add in audio api\n",
    "\n",
    "# 2.2 Prepare track library\n",
    "\n",
    "# LOAD OR INIT TRACK LIBRARY\n",
    "track_library_file = \"cache/trackLibrary.csv\"\n",
    "\n",
    "try:\n",
    "    track_library_df = pd.read_csv(track_library_file)\n",
    "except FileNotFoundError:\n",
    "    track_library_df = pd.DataFrame(columns=[\n",
    "        'track_id', 'track_name', 'album_name', 'album_release_date', 'album_id', 'album_type',\n",
    "        'artist_names', 'artist_ids', 'track_popularity', 'duration_ms', 'explicit', 'isrc',\n",
    "        'ean', 'upc', 'external_urls', 'track_uri'\n",
    "    ])\n",
    "\n",
    "# IDENTIFY NEW TRACKS\n",
    "track_uris = streams['spotify_track_uri'].dropna().astype(str).unique().tolist()\n",
    "track_ids = [uri.split(':')[-1] for uri in track_uris if 'spotify:track:' in uri]\n",
    "all_track_ids = set(track_ids)\n",
    "\n",
    "known_track_ids = set(track_library_df['track_id'].dropna().unique())\n",
    "new_track_ids = all_track_ids - known_track_ids\n",
    "\n",
    "print(f\"{len(all_track_ids)} unique track IDs in streams.\")\n",
    "print(f\"{len(known_track_ids)} tracks in track cache.\")\n",
    "print(f\"{len(new_track_ids)} new tracks to fetch.\")\n",
    "\n",
    "# FETCH TRACK DETAILS\n",
    "tracks_url = \"https://api.spotify.com/v1/tracks\"\n",
    "token_str = token['access_token'] \n",
    "headers = {\"Authorization\": f\"Bearer {token_str}\"}\n",
    "\n",
    "def chunked(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "new_tracks_data = []\n",
    "for i, chunk in enumerate(chunked(list(new_track_ids), 50)):\n",
    "    params = {\"ids\": \",\".join(chunk)}\n",
    "    r = requests.get(tracks_url, headers=headers, params=params)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Request failed with status {r.status_code}, waiting 30s before retrying...\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    response_data = r.json()\n",
    "    tracks = response_data.get('tracks', [])\n",
    "    for t in tracks:\n",
    "        if t:\n",
    "            new_tracks_data.append({\n",
    "                \"track_id\": t.get(\"id\"),\n",
    "                \"track_name\": t.get(\"name\"),\n",
    "                \"album_name\": t.get(\"album\", {}).get(\"name\"),\n",
    "                \"album_release_date\": t.get(\"album\", {}).get(\"release_date\"),\n",
    "                \"album_id\": t.get(\"album\", {}).get(\"id\"),\n",
    "                \"album_type\": t.get(\"album\", {}).get(\"album_type\"),\n",
    "                \"artist_names\": [artist[\"name\"] for artist in t.get(\"artists\", [])],\n",
    "                \"artist_ids\": [artist[\"id\"] for artist in t.get(\"artists\", [])],\n",
    "                \"track_popularity\": t.get(\"popularity\"),\n",
    "                \"duration_ms\": t.get(\"duration_ms\"),\n",
    "                \"explicit\": t.get(\"explicit\"),\n",
    "                \"isrc\": t.get(\"external_ids\", {}).get(\"isrc\"),\n",
    "                \"ean\": t.get(\"external_ids\", {}).get(\"ean\"),\n",
    "                \"upc\": t.get(\"external_ids\", {}).get(\"upc\"),\n",
    "                \"external_urls\": t.get(\"external_urls\", {}),\n",
    "                \"track_uri\": t.get(\"uri\")\n",
    "            })\n",
    "    # checkpoint counts and small delay to reduce rate limit risks\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        print(f\"Fetched metadata for {i * 50} tracks.\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(\"Completed track api calls.\")\n",
    "new_tracks_df = pd.DataFrame(new_tracks_data)\n",
    "print(f\"Fetched metadata for {len(new_tracks_df)} total new tracks.\")\n",
    "\n",
    "if new_tracks_data:\n",
    "    # Append to track_library_df\n",
    "    track_library_df = pd.concat([track_library_df, new_tracks_df], ignore_index=True)\n",
    "    # Remove duplicates just in case\n",
    "    track_library_df.drop_duplicates(subset=['track_id'], inplace=True)\n",
    "    track_library_df.to_csv(track_library_file, index=False)\n",
    "    print(f\"Added {len(new_tracks_data)} new albums to {track_library_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e497cc-9f85-4af8-bae1-2768cb51d8ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deprecated: Ignore this cell\n",
    "\n",
    "# 2.25 Prepare audio library\n",
    "\n",
    "# LOAD OR INIT AUDIO LIBRARY\n",
    "audio_library_file = \"cache/audioLibrary.csv\"\n",
    "\n",
    "try:\n",
    "    audio_library_df = pd.read_csv(audio_library_file)\n",
    "except FileNotFoundError:\n",
    "    audio_library_df = pd.DataFrame(columns=[\n",
    "        'track_id', 'duration_ms', 'analysis_url', 'acousticness',  'danceability',  'energy',\n",
    "        'instrumenalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness',\n",
    "        'tempo', 'time_signature', 'valence'\n",
    "    ])\n",
    "\n",
    "# IDENTIFY NEW AUDIO INFO NEEDED\n",
    "track_uris = streams['spotify_track_uri'].dropna().astype(str).unique().tolist()\n",
    "track_ids = [uri.split(':')[-1] for uri in track_uris if 'spotify:track:' in uri]\n",
    "all_track_ids = set(track_ids)\n",
    "\n",
    "known_track_ids = set(audio_library_df['track_id'].dropna().unique())\n",
    "new_track_ids = all_track_ids - known_track_ids\n",
    "\n",
    "print(f\"{len(all_track_ids)} unique track IDs in streams.\")\n",
    "print(f\"{len(known_track_ids)} tracks in audio cache.\")\n",
    "print(f\"{len(new_track_ids)} new tracks to fetch audio info.\")\n",
    "\n",
    "# FETCH AUDIO DETAILS\n",
    "tracks_url = \"https://api.spotify.com/v1/audio-features\"\n",
    "token_str = token['access_token'] \n",
    "headers = {\"Authorization\": f\"Bearer {token_str}\"}\n",
    "\n",
    "def chunked(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "new_tracks_data = []\n",
    "for i, chunk in enumerate(chunked(list(new_track_ids), 50)):\n",
    "    params = {\"ids\": \",\".join(chunk)}\n",
    "    r = requests.get(tracks_url, headers=headers, params=params)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Request failed with status {r.status_code}, waiting 30s before retrying...\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    response_data = r.json()\n",
    "    tracks = response_data.get('tracks', [])\n",
    "    for t in tracks:\n",
    "        if t:\n",
    "            new_tracks_data.append({\n",
    "                \"track_id\": t.get(\"id\"),\n",
    "                \"duration_ms\": t.get(\"duration_ms\"),\n",
    "                \"analysis_url\": t.get(\"analysis_url\"),\n",
    "                \"acousticness\": t.get(\"acousticness\"),\n",
    "                \"danceability\": t.get(\"danceability\"),\n",
    "                \"energy\": t.get(\"energy\"),\n",
    "                \"instrumenalness\": t.get(\"instrumenalness\"),\n",
    "                \"key\": t.get(\"key\"),\n",
    "                \"liveness\": t.get(\"liveness\"),\n",
    "                \"loudness\": t.get(\"loudness\"),\n",
    "                \"mode\": t.get(\"mode\"),\n",
    "                \"speechiness\": t.get(\"speechiness\"),\n",
    "                \"tempo\": t.get(\"tempo\"),\n",
    "                \"time_signature\": t.get(\"time_signature\"),\n",
    "                \"valence\": t.get(\"valence\")\n",
    "            })\n",
    "    # checkpoint counts and small delay to reduce rate limit risks\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        print(f\"Fetched metadata for {i * 50} tracks.\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(\"Completed track api calls.\")\n",
    "new_tracks_df = pd.DataFrame(new_tracks_data)\n",
    "print(f\"Fetched audio features for {len(new_tracks_df)} total new tracks.\")\n",
    "\n",
    "if new_tracks_data:\n",
    "    # Append to audio_library_df\n",
    "    audio_library_df = pd.concat([audio_library_df, new_tracks_df], ignore_index=True)\n",
    "    # Remove duplicates just in case\n",
    "    audio_library_df.drop_duplicates(subset=['track_id'], inplace=True)\n",
    "    audio_library_df.to_csv(audio_library_file, index=False)\n",
    "    print(f\"Added {len(new_tracks_data)} new albums to {audio_library_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060295d-9f01-4713-b686-34811d05a154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Prepare album library\n",
    "\n",
    "# LOAD OR INIT ALBUM LIBRARY\n",
    "album_library_file = \"cache/albumLibrary.csv\"\n",
    "\n",
    "try:\n",
    "    album_library_df = pd.read_csv(album_library_file)\n",
    "except FileNotFoundError:\n",
    "    album_library_df = pd.DataFrame(columns=[\n",
    "        'album_id', 'album_name', 'album_type', 'total_tracks', 'release_date', \n",
    "        'release_date_precision', 'label', 'album_popularity', 'available_markets',\n",
    "        'album_uri', 'album_external_urls', 'image'\n",
    "    ])\n",
    "\n",
    "# IDENTIFY NEW ALBUMS\n",
    "all_album_ids = set(track_library_df['album_id'].dropna().unique())\n",
    "known_album_ids = set(album_library_df['album_id'].dropna().unique())\n",
    "new_album_ids = all_album_ids - known_album_ids\n",
    "\n",
    "print(f\"{len(all_album_ids)} unique album IDs in track library.\")\n",
    "print(f\"{len(known_album_ids)} albums in album cache.\")\n",
    "print(f\"{len(new_album_ids)} new albums to fetch.\")\n",
    "\n",
    "# FETCH ALBUM DETAILS\n",
    "albums_url = \"https://api.spotify.com/v1/albums\"\n",
    "token_str = token['access_token'] \n",
    "headers = {\"Authorization\": f\"Bearer {token_str}\"}\n",
    "\n",
    "new_albums_data = []\n",
    "for i, chunk in enumerate(chunked(list(new_album_ids), 20)):\n",
    "    params = {\"ids\": \",\".join(chunk)}\n",
    "    r = requests.get(albums_url, headers=headers, params=params)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Albums request failed with {r.status_code}, waiting 30s...\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    albums_json = r.json().get('albums', [])\n",
    "    for alb in albums_json:\n",
    "        if alb:\n",
    "            new_albums_data.append({\n",
    "                \"album_id\": alb.get('id'),\n",
    "                \"album_name\": alb.get('name'),\n",
    "                \"album_type\": alb.get('album_type'),\n",
    "                \"total_tracks\": alb.get('total_tracks'),\n",
    "                \"release_date\": alb.get('release_date'),\n",
    "                \"release_date_precision\": alb.get('release_date_precision'),\n",
    "                \"label\": alb.get('label'),\n",
    "                \"album_popularity\": alb.get('popularity'),\n",
    "                \"available_markets\": alb.get('available_markets'),\n",
    "                \"album_uri\": alb.get('uri'),\n",
    "                \"album_external_urls\": alb.get('external_urls', {}),\n",
    "                'image': alb.get(\"images\", {})\n",
    "            })\n",
    "    # checkpoint counts and small delay to reduce rate limit risks\n",
    "    if i % 25 == 0 and i > 0:\n",
    "        print(f\"Fetched metadata for {i * 20} albums.\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(\"Completed album api calls.\")\n",
    "new_albums_df = pd.DataFrame(new_albums_data)\n",
    "print(f\"Fetched metadata for {len(new_albums_df)} total new albums.\")\n",
    "\n",
    "if new_albums_data:\n",
    "    # Append to album_library_df\n",
    "    album_library_df = pd.concat([album_library_df, new_albums_df], ignore_index=True)\n",
    "    # Remove duplicates just in case\n",
    "    album_library_df.drop_duplicates(subset=['album_id'], inplace=True)\n",
    "    album_library_df.to_csv(album_library_file, index=False)\n",
    "    print(f\"Added {len(new_albums_data)} new albums to {album_library_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff3661-925d-445e-84e6-2b15569b64c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.4 Prepare artist library\n",
    "\n",
    "# LOAD OR INIT ARTIST LIBRARY\n",
    "artist_library_file = \"cache/artistLibrary.csv\"\n",
    "\n",
    "try:\n",
    "    artist_library_df = pd.read_csv(artist_library_file)\n",
    "except FileNotFoundError:\n",
    "    artist_library_df = pd.DataFrame(columns=[\n",
    "        'artist_id', 'artist_name', 'artist_popularity', 'artist_followers',\n",
    "        'artist_genres', 'artist_uri', 'artist_external_urls'\n",
    "    ])\n",
    "\n",
    "# IDENTIFY NEW ARTISTS\n",
    "all_artist_ids = set()\n",
    "for artist_list in track_library_df['artist_ids'].dropna():\n",
    "    if isinstance(artist_list, str):\n",
    "        import ast\n",
    "        artist_list = ast.literal_eval(artist_list)  # convert string \"[...]\" to Python list\n",
    "    all_artist_ids.update(artist_list)\n",
    "\n",
    "known_artist_ids = set(artist_library_df['artist_id'].dropna().unique())\n",
    "new_artist_ids = all_artist_ids - known_artist_ids\n",
    "\n",
    "print(f\"{len(all_artist_ids)} unique artist IDs in track library.\")\n",
    "print(f\"{len(known_artist_ids)} artists in artist cache.\")\n",
    "print(f\"{len(new_artist_ids)} new artists to fetch.\")\n",
    "\n",
    "# FETCH ARTIST DETAILS\n",
    "artists_url = \"https://api.spotify.com/v1/artists\"\n",
    "new_artists_data = []\n",
    "for i, chunk in enumerate(chunked(list(new_artist_ids), 50)):\n",
    "    params = {\"ids\": \",\".join(chunk)}\n",
    "    r = requests.get(artists_url, headers=headers, params=params)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Artists request failed with {r.status_code}, waiting 30s...\")\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "    artists_json = r.json().get('artists', [])\n",
    "    for art in artists_json:\n",
    "        if art:\n",
    "            new_artists_data.append({\n",
    "                \"artist_id\": art.get('id'),\n",
    "                \"artist_name\": art.get('name'),\n",
    "                \"artist_popularity\": art.get('popularity'),\n",
    "                \"artist_followers\": art.get('followers', {}).get('total'),\n",
    "                \"artist_genres\": art.get('genres', []),\n",
    "                \"artist_uri\": art.get('uri'),\n",
    "                \"artist_external_urls\": art.get('external_urls', {})\n",
    "            })\n",
    "    # checkpoint counts and small delay to reduce rate limit risks\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        print(f\"Fetched metadata for {i * 50} artists.\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(\"Completed artist api calls.\")\n",
    "new_artists_df = pd.DataFrame(new_artists_data)\n",
    "print(f\"Fetched metadata for {len(new_artists_df)} total new artists.\")\n",
    "\n",
    "if new_artists_data:\n",
    "    # Append to artist_library_df\n",
    "    artist_library_df = pd.concat([artist_library_df, new_artists_df], ignore_index=True)\n",
    "    # Remove duplicates just in case\n",
    "    artist_library_df.drop_duplicates(subset=['artist_id'], inplace=True)\n",
    "    artist_library_df.to_csv(artist_library_file, index=False)\n",
    "    print(f\"Added {len(new_artists_data)} new artists to {artist_library_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b31de3-52a1-42e6-b1bd-2052c91162ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.5 Merge streams with libraries\n",
    "\n",
    "# Ensure streams has a track_id column for merging\n",
    "streams['track_id'] = streams['spotify_track_uri'].astype(str).apply(\n",
    "    lambda x: x.split(':')[-1] if 'spotify:track:' in x else x\n",
    ")\n",
    "\n",
    "with_track_info = streams.merge(track_library_df, on='track_id', how='left')\n",
    "with_album_info = with_track_info.merge(album_library_df, on='album_id', how='left')\n",
    "# with_artist_info = with_album_info.merge(artist_library_df, on='artist_id', how='left') # multi-artist will make this an issue\n",
    "# with_audio_info = with_artist_info.merge(audio_library_df, on='track_id', how='left')\n",
    "\n",
    "print(\"Enriched streams DataFrame:\")\n",
    "display(with_album_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96351c17-ebf7-4e82-8ce5-5e1b6c06549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Graph and display data insights\n",
    "\n",
    "# 3.1 Lifetime rankings by streams\n",
    "\n",
    "# Top Artists by Play Count\n",
    "top_artists = streams['master_metadata_album_artist_name'].value_counts().head(20)\n",
    "# print(\"Top 20 Artists by Play Count:\")\n",
    "# print(top_artists)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "top_artists.plot(kind='bar')\n",
    "plt.title(\"Top 20 Artists by Number of Streams\")\n",
    "plt.show()\n",
    "\n",
    "# Top Albums by Play Count\n",
    "top_albums = streams['master_metadata_album_album_name'].value_counts().head(20)\n",
    "# print(\"Top 20 Albums by Play Count:\")\n",
    "# print(top_albums)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "top_albums.plot(kind='bar')\n",
    "plt.title(\"Top 20 Albums by Number of Streams\")\n",
    "plt.show()\n",
    "\n",
    "# Top Songs by Play Count\n",
    "top_songs = streams['master_metadata_track_name'].value_counts().head(20)\n",
    "# print(\"Top 20 Songs by Play Count:\")\n",
    "# print(top_songs)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "top_songs.plot(kind='bar')\n",
    "plt.title(\"Top 20 Songs by Number of Streams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b305d-b5bf-47ee-9f97-98ba2fd6f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Lifetime rankings by duration\n",
    "\n",
    "# Top Artists by Total Play Duration\n",
    "top_artists = streams.groupby('master_metadata_album_artist_name')['ms_played'].sum().div(3600000).sort_values(ascending=False).head(20)\n",
    "# print(\"Top 20 Artists by Total Play Duration (hours):\")\n",
    "# print(top_artists)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_artists.plot(kind='bar')\n",
    "plt.title(\"Top 20 Artists by Total Play Duration\")\n",
    "plt.xlabel(\"Artist\")\n",
    "plt.ylabel(\"Play Duration (hours)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top Albums by Total Play Duration\n",
    "top_albums = streams.groupby('master_metadata_album_album_name')['ms_played'].sum().div(3600000).sort_values(ascending=False).head(20)\n",
    "# print(\"Top 20 Albums by Total Play Duration (hours):\")\n",
    "# print(top_albums)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_albums.plot(kind='bar')\n",
    "plt.title(\"Top 20 Albums by Total Play Duration\")\n",
    "plt.xlabel(\"Album\")\n",
    "plt.ylabel(\"Play Duration (hours)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top Songs by Total Play Duration\n",
    "top_songs = streams.groupby('master_metadata_track_name')['ms_played'].sum().div(3600000).sort_values(ascending=False).head(20)\n",
    "# print(\"Top 20 Songs by Total Play Duration (hours):\")\n",
    "# print(top_songs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_songs.plot(kind='bar')\n",
    "plt.title(\"Top 20 Songs by Total Play Duration\")\n",
    "plt.xlabel(\"Song\")\n",
    "plt.ylabel(\"Play Duration (hours)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed487e-64dc-4876-8b50-6c776d5333d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Lifetime listening durations\n",
    "\n",
    "# Create a helper column for hours played\n",
    "streams['hours_played'] = streams['ms_played'] / (1000 * 60 * 60)  # Convert ms to hours\n",
    "\n",
    "# Set the index to the datetime column so resample works cleanly\n",
    "streams.set_index('ts', inplace=True)\n",
    "\n",
    "# --- For Number of Streams ---\n",
    "\n",
    "# Resample to monthly frequency, counting the number of streams per month\n",
    "monthly_counts = streams.resample('ME').size()\n",
    "\n",
    "# Calculate 12-month rolling average of the monthly counts\n",
    "counts_12m_avg = monthly_counts.rolling(12).mean()\n",
    "\n",
    "# --- For Total Duration ---\n",
    "\n",
    "# Resample to monthly frequency, summing the hours played per month\n",
    "monthly_duration = streams['hours_played'].resample('ME').sum()\n",
    "\n",
    "# Calculate 12-month rolling average of monthly duration\n",
    "duration_12m_avg = monthly_duration.rolling(12).mean()\n",
    "\n",
    "# --- Plotting ---\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the 12-month rolling average of number of streams\n",
    "ax = counts_12m_avg.plot(label='12-Month Rolling Avg of Streams', color='blue')\n",
    "\n",
    "# Secondary axis for total duration\n",
    "ax2 = ax.twinx()\n",
    "duration_12m_avg.plot(ax=ax2, label='12-Month Rolling Avg of Duration (hours)', color='green', linestyle='--')\n",
    "\n",
    "# Labeling\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Streams')\n",
    "ax2.set_ylabel('Total Duration (hours)')\n",
    "plt.title(\"12-Month Rolling Average: Number of Streams & Total Play Duration\")\n",
    "\n",
    "# Combine legends\n",
    "lines_1, labels_1 = ax.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588ae7f-f436-45fd-be92-b35bedfba456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Top 10 artists over time\n",
    "\n",
    "# Convert ms to hours if not already done\n",
    "streams['hours_played'] = streams['ms_played'] / (1000.0 * 60 * 60)\n",
    "\n",
    "# Identify the top 10 artists\n",
    "artist_totals = streams.groupby('master_metadata_album_artist_name')['hours_played'].sum()\n",
    "top10_artists = artist_totals.nlargest(10).index\n",
    "\n",
    "# Filter to those artists\n",
    "top10_data = streams[streams['master_metadata_album_artist_name'].isin(top10_artists)]\n",
    "\n",
    "# Group by month & artist, sum hours_played\n",
    "monthly_artist = top10_data.groupby([pd.Grouper(freq='ME'), 'master_metadata_album_artist_name'])['hours_played'].sum()\n",
    "monthly_artist_pivot = monthly_artist.unstack('master_metadata_album_artist_name')\n",
    "\n",
    "# Compute 12-month rolling average\n",
    "monthly_artist_12m = monthly_artist_pivot.rolling(12).mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_artist_12m.plot(ax=plt.gca())\n",
    "plt.title(\"12-Month Rolling Avg of Hours Played for Top 10 Artists\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Hours Played (Monthly)\")\n",
    "plt.legend(title=\"Artist\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb1aaa-fc19-4fe2-86be-1b250b244c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Custom artist line\n",
    "\n",
    "# Convert ms to hours if not already done\n",
    "streams['hours_played'] = streams['ms_played'] / (1000.0 * 60 * 60)\n",
    "\n",
    "artists = ['Attack Attack!']\n",
    "# Filter to those artists\n",
    "top10_data = streams[streams['master_metadata_album_artist_name'].isin(artists)]\n",
    "\n",
    "# Group by month & artist, sum hours_played\n",
    "monthly_artist = top10_data.groupby([pd.Grouper(freq='ME'), 'master_metadata_album_artist_name'])['hours_played'].sum()\n",
    "monthly_artist_pivot = monthly_artist.unstack('master_metadata_album_artist_name')\n",
    "\n",
    "# Compute 12-month rolling average\n",
    "monthly_artist_12m = monthly_artist_pivot.rolling(12).mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_artist_12m.plot(ax=plt.gca())\n",
    "plt.title(\"12-Month Rolling Avg of Hours Played for Top 10 Artists\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Hours Played (Monthly)\")\n",
    "plt.legend(title=\"Artist\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bf183-5e39-4864-8220-f7e66a799b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Monthly topline\n",
    "\n",
    "streams['month'] = streams.index.to_period('M')\n",
    "\n",
    "# Group by month and collect results in a list\n",
    "summary_rows = []\n",
    "for month_period, group_df in streams.groupby('month'):\n",
    "    # Total stream time (hours) for this month\n",
    "    total_stream_time = group_df['hours_played'].sum()\n",
    "\n",
    "    # Top artist for this month (by total hours)\n",
    "    # Group by artist, sum hours_played, pick the max\n",
    "    artist_sums = group_df.groupby('master_metadata_album_artist_name')['hours_played'].sum().sort_values(ascending=False)\n",
    "    top_artist = artist_sums.index[0] if not artist_sums.empty else None\n",
    "\n",
    "    # Top album for this month\n",
    "    album_sums = group_df.groupby('master_metadata_album_album_name')['hours_played'].sum().sort_values(ascending=False)\n",
    "    top_album = album_sums.index[0] if not album_sums.empty else None\n",
    "\n",
    "    # Top song for this month\n",
    "    song_sums = group_df.groupby('master_metadata_track_name')['hours_played'].sum().sort_values(ascending=False)\n",
    "    top_song = song_sums.index[0] if not song_sums.empty else None\n",
    "\n",
    "    summary_rows.append([\n",
    "        month_period,\n",
    "        top_artist,\n",
    "        top_album,\n",
    "        top_song,\n",
    "        total_stream_time\n",
    "    ])\n",
    "\n",
    "# Create a summary DataFrame from the rows\n",
    "summary_df = pd.DataFrame(summary_rows, columns=[\n",
    "    'month',\n",
    "    'top_artist',\n",
    "    'top_album',\n",
    "    'top_song',\n",
    "    'total_stream_time_hours'\n",
    "])\n",
    "\n",
    "# Convert the 'month' Period to a timestamp (end of month) for clarity, or keep as Period\n",
    "summary_df['month'] = summary_df['month'].dt.to_timestamp(how='end')\n",
    "\n",
    "# Sort by month chronologically\n",
    "summary_df.sort_values('month', inplace=True)\n",
    "summary_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove limits on rows and columns shown\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Now any display(df) call will show all rows and columns\n",
    "display(summary_df)\n",
    "\n",
    "# Reset to defaults limits\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
